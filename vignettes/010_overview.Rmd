---
title: "Overview"
author:
- Department for Environment and Water
- Nigel Willoughby
date: "`r format(Sys.time(), '%A, %d %B, %Y')`"
output: rmarkdown::html_vignette
bibliography: ["packages.bib", "refs.bib", "gbif.bib"]
vignette: >
  %\VignetteIndexEntry{Overview}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}

  knitr::opts_knit$set(root.dir = here::here())

  knitr::opts_chunk$set(collapse = TRUE
                        , comment = "#>"
                        , message = FALSE
                        , warning = FALSE
                        )

```

```{r setup, warning = FALSE}

  pacs <- c("knitr"
            , "envClean"
            , "envReport"
            , "envFunc", "fs", "purrr"
            , "dplyr", "sf", "tibble"
            , "tmap", "raster", "rstanarm"
            )

  purrr::walk(pacs
              , ~suppressPackageStartupMessages(library(.
                                                        , character.only = TRUE
                                                        , quietly = TRUE)
                                                )
              )

  #  Load data
  flor_all <- tibble::as_tibble(envClean::flor_all)
  
  # What crs to use for maps?
  use_crs <- 3577 # actually an epsg code. see epsg.io
  
  # set area of interest coordinate reference system
  aoi <- envClean::aoi %>%
    sf::st_transform(crs = use_crs)
  
```

```{r housekeeping, include = FALSE, echo = FALSE}

  pac_cits <- "vignettes/packages.bib"

  unlink(pac_cits)

  tmap_mode("plot")
  
  knitr::write_bib(file = pac_cits)

```

## Installation

`envClean` is not on CRAN.

Install the development version from GitHub

```r
remotes::install_github("acanthiza/envClean")
```

Load `envClean`

```r
library("envClean")
```

## Suggested workflow {#workflow}

After many, many iterations, the following workflow has been found to be ok. Only ok. There is no awesome when cleaning large, unstructured data.

```{r workflow, echo = FALSE}
  
  knitr::kable(envClean::lufilter
        , caption = "Suggested steps in the cleaning process"
        )

```

## Key concepts

### Filter/clean/tidy

`envClean`, helps with implementing:

* filtering: remove rows of a data frame. These may be entirely legitimate observations but it is desirable to remove them for the purposes of a downstream analysis. For example, a [context] with only one (legitimate) record may not meet the expectations of an analysis that within each [context] there is a list of taxa recorded.
* cleaning: remove observations to reduce the risk that spurious observations are included in downstream analysis. For example, two different data sources may contain the same observation. Most analyses will perform better when records duplicated within a context are removed.
* tidying: as per [tidy data](https://www.jstatsoft.org/article/view/v059i10) [@JSSv059i10] where each variable is a column and each observation is a unique row.

In practice these tasks are often blurred within each of the functions. For example `filter_taxa` cleans taxonomic names and tidies the data set with respect to any [context] specified.

In general the process will be referred to as _cleaning_.

### Site/visit/record/taxa

Due to the loose definition of [context](#context) (see below), the definitions of site, visit, record and taxa can change through the cleaning process. 

* sites are spatial locations. they may be defined by latitude, longitude, easting, northing and/or cell. These may be duplicated before exclusive application of context. They are not necessarily defined by all spatial concepts within context at all stages of the cleaning process.
* visits are sites plus a time, such as year, month, day (or, even hour). Again, until context is applied exclusively, these may be duplicated
* records are visits plus an observation to some level of the taxonomic hierarchy (refered to simply as 'taxa')
* taxa refers to some form of taxonomic entity. An entity may be duplicated within a visit before taxonomy is resolved and context is applied exclusively (usually by `filter_taxa`).

### Context and binning {#context}

Context is usually defined by two scales of particular interest: space (location) and time. It is possible, but often not desirable, to assign other concepts to `context` too (e.g. say, data source).

Throughout the series of `env` packages the concept of _context_ is used extensively, and at least currently, somewhat loosely. Context is also related to `binning`: lumping records into spatial and or temporal 'bins'. Later in the cleaning process, spatial and temporal bins are likely to define the context.

With respect to 'loosely': context may be defined by, say, `c("lat", "long", "cell", "year", "month")`. At various stages through the cleaning process not every one of those variables may be applicable. After running `add_raster_cell` (to assign a spatial bin) the variable `lat` and `long` may be removed (depending on the `add_xy` argument). However `context` can still be used in full in cleaning steps (via the consistent use of `tidyselect::any_of` in `envClean` functions).

Note that `context` is only applied _exclusively_ only when `filter_taxa` is run (see examples below). Thus: extraneous fields/columns beyond `context` are maintained until taxonomy is filtered (unless they are provided via the various arguments to `filter_taxa`); and no claim is made regarding the uniqueness of 'records' until this step in the process. Thus, if a cleaning process does not include `filter_taxa`, running, say, `dplyr::distinct(across(any_of(context)))` may be useful.

```{r context_example}

  # Simple context example
  flor_example <- tibble::tribble(
     ~data_source, ~original_name, ~location, ~year,
     "A", "Eucalyptus leucoxylon", "x", "y",
     "B", "Eucalyptus leucoxylon", "x", "y"
   )

  flor_example

  # set context 
  context <- c("location", "year")
  
  # filter singletons - context not applied
  flor_single <- flor_example %>%
    filter_counts(context = context
                  , thresh = 1
                  )
  
  # two rows
  flor_single
      
  # filter taxa - context applied exclusively
  flor_taxa <- flor_example %>%
    filter_taxa(context = context)
  
  # one row
  flor_taxa

```

### Naming

By careful use of naming when creating objects during the cleaning process it is possible to create automated summaries of the process. See the [workflow](#workflow) section for suggested names for steps in the cleaning process. Within this vignette, the prefix `flor_` has been added and the function `add_time_stamp()` has been used to assist ordering the cleaning process [summary](#summary).

### Coordinate reference systems

There are two (possibly three) main coordinate reference systems (crs) to worry about:

1. the crs for the original records. If these are in decimal degrees, using [epsg](https://epsg.io/) = [4283](https://epsg.io/4283) is likely to return the correct crs.
2. the crs you'd like to use for most spatial data. Set here (in setup chunk) to `use_crs` = `r use_crs`. It is likely that a projected crs will work best, particularly for buffering, filtering etc.
3. the crs for any other spatial data imported to help with cleaning. Try using `sf::st_read("random_shape_file.shp") %>% sf::st_tranform(crs = use_crs)` to deal with this.


## Import

Querying and uniting disparate data sources into a single data set is a challenge in its own right. See [envImport](https://github.com/Acanthiza/envImport) for tools to assist there. Once you've imported and combined all your data, read on.

## References
