---
title: "envClean"
author:
- Department for Environment and Water
- Nigel Willoughby
date: "`r format(Sys.time(), '%A, %d %B, %Y. %H:%M')`"
output: rmarkdown::html_vignette
bibliography: ["packages.bib", "refs.bib"]
vignette: >
  %\VignetteIndexEntry{envClean}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}

  knitr::opts_knit$set(root.dir = here::here())

  knitr::opts_chunk$set(
    collapse = TRUE
    , comment = "#>"
  )

```

```{r setup}

  library(knitr)
  library(envClean)
  library(envReport)
  library(envFunc) # needed for add_raster_cell
  library(fs)
  library(purrr)
  library(dplyr)
  library(sf)
  library(tibble)
  library(tmap)
  library(raster)
  library(rstanarm)

  fix_bib(bib_file = "vignettes/packages.bib")

  tmap_mode("plot")

  flor_all <- tibble::as_tibble(flor_all)
  
  # What crs to use for maps?
  use_crs <- 3577 # actually an epsg code. see epsg.io
  
  aoi <- aoi %>%
    sf::st_transform(crs = use_crs)
  
```

## Get `envClean`

`envClean` is not on CRAN.

Install the development version from GitHub

```r
remotes::install_github("acanthiza/envClean")
```

Load `envClean`

```r
library("envClean")
```
## Suggested workflow

After many, many iterations, the following workflow has been found to be ok. Only ok. There is no awesome when cleaning large, unstructured data.

### Import

Querying and uniting disparate data sources into a single data set is a challenge in its own right. See `envImport` for tools to assist there. Once you've imported and combined all your data, read on.

Here, we'll start with combined floristic data (called `flor_all`) from the mallee in South Australia. This data set is provided with `envClean`.

```{r flor_all}

  flor_all

```

### Note on coordinate reference systems

There are two (possibly three) main coordinate reference systems (crs) to worry about:

1. the crs for the original records. If these are in decimal degrees, using [epsg](https://epsg.io/) = [4283](https://epsg.io/4283) is likely to return the correct crs.
2. the crs you'd like to use for most spatial data. Set here (in setup chunk) to `use_crs` = `r use_crs`. It is likely that a projected crs will work best, particularly for buffering, filtering etc.
3. the crs for any other spatial data imported to help with cleaning. Try using `sf::st_read("random_shape_file.shp") %>% sf::st_tranform(crs = use_crs)` to deal with this.

### Area of interest

Usually this is a geographic and/or taxonomic area of interest. An example area overlapping `flor_all` is provided in `aoi`. Converting `flor_all` to `sf` allows plotting them together.

```{r aoi}

  flor_all_sf <- flor_all %>%
    sf::st_as_sf(coords = c("long", "lat")
                 , crs = 4326
                 )

  tm_shape(aoi
           , bbox = st_bbox(flor_all_sf)
           ) +
    tm_polygons() +
  tm_shape(flor_all_sf) +
    tm_dots()

```

Filtering `flor_all` to `aoi` is done with `filter_aoi`.

```{r flor_all_aoi}

  flor_aoi <- filter_aoi(flor_all
                         , use_aoi = aoi
                         , crs_aoi = st_crs(aoi)
                         ) %>%
    add_time_stamp()

  flor_aoi

```

Check that spatial filter worked.

```{r flor_aoi}

  flor_aoi_sf <- flor_aoi %>%
    sf::st_as_sf(coords = c("long", "lat")
                 , crs = 4326
                 )

  tm_shape(aoi
           , bbox = st_bbox(flor_all_sf)
           ) +
    tm_polygons() +
  tm_shape(flor_aoi_sf) +
    tm_dots()

```

## Context: scales of interest {#context}

There are two scales of particular interest: space (location) and time. Together these are used to set the `context` throughout filtering and analysis. It is possible to assign other concepts to `context` too (e.g. say, data source). Note that `context` is first applied _exclusively_ only when taxonomy is filtered. For example, if sppA has a record at location x and time y from different data sources (see table below) these will only be reduced to a single 'record' when taxonomy is filtered. Another way to say this: extraneous fields/columns beyond `context` are maintained until taxonomy is filtered. However, even at that point, extra columns can be specified (see [Filter taxonomy]).

The original location columns probably suggest metre accuracy, or even sub-metre. There may also be a field dampening expectations of such accuracy with estimates of precision for the location. In the following workflow, a precision threshold is set and then an accuracy threshold is adopted. All records with worse precision than threshold are removed, and then all records within the accuracy threshold are lumped. The lumping is done via a raster placed over the `aoi`.

The original time scale probably suggests accuracy to day, or perhaps even hour, or sub-hour. Choose a scale of relevance to your question. In the example below month is used. Thus all data recorded within a spatial location within a month are treated as one 'visit'. A 'taxa' within a 'visit' is considered a 'record'.

```{r context_example, echo = FALSE}

  context_example <- tibble::tribble(
     ~data_source, ~taxa, ~location, ~year,
     "A", "sppA", "x", "y",
     "B", "sppA", "x", "y"
   )

  kable(context_example, caption = "Context example")

```

### Precision

Records with precision less than threshold are filtered using `filter_spat_rel`. This takes a dataframe (`df`) as its first argument, in this case `flor_aoi`. `dist_col` specifies the column in `df` that contains the precision estimates. `dist` provides the threshold above which to filter. If there are data sources (or any other columns in `df`) that do not include an estimate of spatial precision, but you would like to keep, this can be done with the argument `over_ride`. This takes a named list, where names need to match the columns in `df`. Any levels within the columns provided in `over_ride` will not be filtered, irrespective of the values in `dist_col`.

```{r flor_aoi_rel}

  context <- c("year", "month", "lat", "long", "cell")

  include_data_name <- c("ALIS","BCM","NVB","TERN")

  flor_rel <- filter_spat_rel(flor_aoi
                              , dist_col = "rel_dist"
                              , dist = 100
                              , context = context
                              , over_ride = list(data_name = include_data_name)
                              ) %>%
    add_time_stamp()
  
  flor_rel

```

### Rasterize

Now that records with dubious spatial precision have been removed, an accuracy threshold is adpoted by rasterizing remaining records into the cells of `aoi_raster`, created here.

```{r aoi_raster}

  aoi_raster <- raster(ext = round(extent(aoi), -3)
                      , resolution = 30
                      , crs = CRS(paste0("+init=epsg:",use_crs))
                      )

  aoi_raster

```

Rasterizing the current data is then done via `add_raster_cell`. This function has the argument `add_xy` which will add the centroid of the cell back to the data frame using the same names as the original `x` and `y` columns. Alternatively, the `x` and `y` columns will be lost from the returned data frame, replaced with `cell`, the raster cell id.

```{r flor_rel_cell}

  flor_cell <- add_raster_cell(aoi_raster
                               , flor_rel
                               , add_xy = TRUE
                               , crs_df = 4283
                               ) %>%
    add_time_stamp()

  flor_cell

```

## Taxonomy

### Make taxonomy

Historically, reconciling taxonomy has been by far the most time-consuming, and necessarily expert-driven part of cleaning biological data from any unstructured data set(s). In the past it was often necessary to get an expert botanist to generate an analysis specific taxonomy, taking into account such things as the area of interest, timing (e.g. decade (was it a wet decade), season (was the timing right for orchids or not?)) of the of the main previous surveys, and even possibly the expertise-at-survey-time of the observers. Nothing will replace the quality of the results from such a process. Using online taxonomic tools such as the [GBIF taxonomy backbone](https://www.gbif.org/dataset/d7dddbf4-2cf0-4f39-9b2a-bb099caae36c) provides an automated alternative that comes with an unquantified (but unlikely to be negligible) penalty in quality. However, the enormous benefit in flexibility and time provided by automated tools makes them essential in practice.

Reconciling taxonomy can be done with the `make_taxa_taxonomy` function. This creates two objects that are loaded to the global environment:

* `lutaxa.` This is a simple lookup from the original name provided to the taxa to use in place of that name. Thus, `taxa` is likely to be duplicated
* `taxa_taxonomy` provides the full taxonomic hierarchy for each `taxa` in the `lutaxa` object. The column `taxa` should not have any duplicates. The output can also include lifespan and indigenous status columns, if they are available from any of the original data sources and they are provided to `make_taxa_taxonomy`.

```{r make_taxa_taxonomy}

  make_taxa_taxonomy(flor_cell
                     , save_luGBIF = path("vignettes","luGBIF.feather")
                     )

```

`lutaxa` is returned to the global environment.

```{r lutaxa}

  lutaxa

```

as is `taxa_taxonomy`

```{r taxa_taxonomy}

  taxa_taxonomy

```

### Filter taxonomy

Cleaning to a single taxonomy is now possible sing `lutaxa` and `taxa_taxonomy`. At this point, the [context](#context) is also _exclusively_ applied. Thus the output of applying `filter_taxa` is to generate a single record for each taxa within each context, with a single field-based result for any `extra_cols` provided to `filter_taxa`. Likewise, by providing `do_cov` and `lucov` to `filter_taxa`, a single, field-based estimate of cover is generated for each 'record'.

```{r filt_tax}

  flor_taxa <- filter_taxa(flor_cell
                          , taxa_col = "original_name"
                          , context = context
                          , do_cov = TRUE
                          , lucov = envEcosystems::lucover
                          ) %>%
    add_time_stamp()

  flor_taxa

```


## Singletons

An implicit assumption in collation of data sources is usually that records were a list of taxa collected at a specific spatial location (and date). In many cases this assumption proves incorrect. For example, records may be related to, say, tree health monitoring where no other taxa were concurrently recorded.

Thus, filtering 'singleton' sites (a site with only a single taxa was recorded) is often prudent.

This will also inadvertently filter legitimate single taxa lists. For example, some areas of samphire or mangroves may have only a single taxa recorded within a survey site. As always, it depends on the goal of any particular analysis whether this trade-off will be worthwhile.

```{r flor_single}
  
  flor_single <- filter_counts(flor_taxa
                              , context = context
                              , thresh = 1
                              ) %>%
    add_time_stamp()

  flor_single

```

## Effort

Aggregated data sets are likely to contain records of taxa at spatial locations collected by almost any method imaginable. The effort any observer(s) put into time and taxonomy at each spatial location is usually unknown.

Given that effort matters when documenting ecology [e.g. @RN2377], filtering 'effort' is an attempt to remove the most seriously under- and over-sampled contexts.

Examples of low taxa richness contexts that may occur in large, unstructured data sets:

* brief, opportune records
* tree-health monitoring data

Examples of high taxa richness contexts that may occur in large, unstructured data sets:

* an observer wandering widely from the location they recorded, particularly if crossing an ecotone
* several observers working together
* an observer with well above average botanical knowledge
* observations taken over a long time-frame but recorded on a single day

These examples just scratch the surface of the ways in which taxa richness can deviate from that expected from an average effort by an average observer.

Taxa richness can be modelled as a function of any variables of interest, for example:

* nothing. This option might be useful in relatively small areas where the non-biological drivers of biological change are relatively stable
* a continuous variable of interest. Say, annual mean temperature, or, soil ph
* principal components. This option is best suited to analyses accompanied by many variables all of which are drivers, or closely correlated with, biological change
* a categorical variable of interest. Say, [IBRA](https://www.awe.gov.au/agriculture-land/land/nrs/science/ibra) Subregions

Effort is filtered using the `filter_effort` function. This models taxa richness using a Bayesian generalised linear model implemented by the `rstan_glm` function in the `rstanarm` [`r cite_package("rstanarm", pac_cite_file = "vignettes/packages.bib")`] package. The model is used to predict upper and lower bounds for taxa richness. Any cell with taxa richness outside those bounds can be removed, by setting the acceptable lower and upper percentiles.

```{r flor_effort}

  # effort_mod <- make_effort_mod(flor_single
  #                               , context = context
  #                               )
  # 
  # flor_effort <- flor_single %>%
  #   dplyr::inner_join(effort_mod$mod_cell_result %>%
  #                       dplyr::filter(keep)
  #                     )
  # 
  # flor_effort

```

## Filtering summary


```{r filt_summary}
  
  filt_summary <- ls(pattern = "flor_") %>%
    grep("combine|all|wide|sf", ., value = TRUE, invert = TRUE) %>%
    tibble::enframe(name = NULL, value = "name") %>%
    dplyr::mutate(obj = map(name,get)
                  , has_stamp = map_lgl(obj,~"ctime" %in% names(attributes(.)))
                  ) %>% 
    dplyr::filter(has_stamp) %>%
    dplyr::mutate(nrow = map_dbl(obj, nrow)) %>%
    dplyr::mutate(ctime = map(obj,attr,"ctime")) %>%
    tidyr::unnest(cols = c(ctime)) %>%
    dplyr::arrange(desc(nrow),ctime)


```


## References

```{r pacCitations, include = FALSE}

  

```
