---
title: "The full process"
author:
- Department for Environment and Water
- Nigel Willoughby
date: "`r format(Sys.time(), '%A, %d %B, %Y')`"
output: rmarkdown::html_vignette
bibliography: ["packages.bib", "refs.bib", "gbif.bib"]
vignette: >
  %\VignetteIndexEntry{The full process}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---


```{r, include = FALSE}

  knitr::opts_knit$set(root.dir = here::here())

  knitr::opts_chunk$set(collapse = TRUE
                        , comment = "#>"
                        , message = FALSE
                        , warning = FALSE
                        )

```

```{r setup, warning = FALSE}

  pacs <- c("knitr"
            , "envClean"
            , "envReport"
            , "envFunc", "fs", "purrr"
            , "dplyr", "sf", "tibble"
            , "tmap", "raster", "rstanarm"
            )

  purrr::walk(pacs
              , ~suppressPackageStartupMessages(library(.
                                                        , character.only = TRUE
                                                        , quietly = TRUE)
                                                )
              )

  #  Load data
  flor_all <- tibble::as_tibble(envClean::flor_all)
  
  # What crs to use for maps?
  use_crs <- 3577 # actually an epsg code. see epsg.io
  
  # set area of interest coordinate reference system
  aoi <- envClean::aoi %>%
    sf::st_transform(crs = use_crs)
  
```

```{r housekeeping, include = FALSE, echo = FALSE}

  pac_cits <- "vignettes/packages.bib"

  unlink(pac_cits)

  tmap_mode("plot")
  
  knitr::write_bib(file = pac_cits)

```

# Data

Here, we'll start with combined floristic data (called `flor_all`) from the mallee in South Australia. This data set is provided with `envClean` and is a small subset of a [GBIF](https://www.gbif.org/) occurrence download [@GBIFRef_6].

```{r flor_all}

  flor_all <- flor_all %>% tibble::as_tibble()

```

# Area of interest

Usually this is a geographic and/or taxonomic area of interest. An example area overlapping `flor_all` is provided in `aoi`. Converting `flor_all` to `sf` allows plotting them together.

```{r aoi}

  # Create simple feature from flor_all
  flor_all_sf <- flor_all %>%
    sf::st_as_sf(coords = c("long", "lat")
                 , crs = 4326
                 )

  # Plot flor_all along with aoi
  tm_shape(aoi
           , bbox = st_bbox(flor_all_sf)
           ) +
    tm_polygons() +
  tm_shape(flor_all_sf) +
    tm_dots()

```

Filtering `flor_all` to `aoi` is done with `filter_geo_range`.

```{r flor_geo_range}

  # Filter area of interest
  flor_geo_range <- filter_geo_range(flor_all
                         , use_aoi = aoi
                         , crs_aoi = st_crs(aoi)
                         ) %>%
    add_time_stamp()

  # Have a look at results
  flor_geo_range

```

Check that spatial filter worked.

```{r flor_geo_range_map}

  # Create sf from flor_geo_range
  aoi_sf <- flor_geo_range %>%
    sf::st_as_sf(coords = c("long", "lat")
                 , crs = 4326
                 )

  # Plot flor_geo_range along with aoi (using the same extent/bbox as the previous plot)
  tm_shape(aoi
           , bbox = st_bbox(flor_all_sf)
           ) +
    tm_polygons() +
  tm_shape(aoi_sf) +
    tm_dots()

```

# Context: scales of interest {#context}



The original location columns probably suggest metre accuracy, or even sub-metre. There may also be a field dampening expectations of such accuracy with estimates of precision for the location. In the following workflow, a precision threshold is set and then an accuracy threshold is adopted. All records with worse precision than threshold are removed, and then all records within the accuracy threshold are lumped. The lumping is done via a raster placed over the `aoi`.

The original time scale probably suggests accuracy to day, or perhaps even hour, or sub-hour. Choose a scale of relevance to your question. In the example below month is used. Thus all data recorded within a spatial location within a month are treated as one 'visit'. A 'taxa' within a 'visit' is considered a 'record'.



## Precision

Records with precision less than threshold are filtered using `filter_geo_rel`. This takes a dataframe (`df`) as its first argument, in this case `flor_geo_range`. `dist_col` specifies the column in `df` that contains the precision estimates. `dist` provides the threshold above which to filter. If there are data sources (or any other columns in `df`) that do not include an estimate of spatial precision, but you would like to keep, this can be done with the argument `over_ride`. This takes a named list, where names need to match the columns in `df`. Any levels within the columns provided in `over_ride` will not be filtered, irrespective of the values in `dist_col`.

```{r flor_geo_rel}

  context <- c("year", "month", "lat", "long", "cell")

  include_data_name <- c("ALIS","BCM","NVB","TERN")

  flor_geo_rel <- filter_geo_rel(flor_geo_range
                              , dist_col = "rel_metres"
                              , dist = 100
                              , context = context
                              , over_ride = list(data_name = include_data_name)
                              ) %>%
    add_time_stamp()
  
  # Have a look at results
  flor_geo_rel

```

## Rasterize

Now that records with dubious spatial precision have been removed, an accuracy threshold is adpoted by rasterizing remaining records into the cells of `aoi_raster`, created here.

```{r aoi_raster}

  aoi_raster <- terra::rast(ext = round(terra::ext(aoi), -3)
                      , resolution = 30
                      , crs = paste0("epsg:",use_crs)
                      )


  aoi_raster

```

Rasterizing the current data is then done via `add_raster_cell`. This function has the argument `add_xy` which will add the centroid of the cell back to the data frame using the same names as the original `x` and `y` columns. Alternatively, the `x` and `y` columns will be lost from the returned data frame, replaced with `cell`, the raster cell id.

```{r flor_geo_bin}

  flor_geo_bin <- envRaster::add_raster_cell(aoi_raster
                               , flor_geo_rel
                               , add_xy = TRUE
                               , crs_df = 4283
                               ) %>%
    add_time_stamp()

  # Have a look at results
  flor_geo_bin

```

# Taxonomy

## Make taxonomy

Historically, reconciling taxonomy has been by far the most time-consuming, and necessarily expert-driven part of cleaning biological data from any unstructured data set(s). In the past it was often necessary to get an expert botanist to generate an analysis specific taxonomy, taking into account such things as the area of interest, timing (e.g. decade (was it a wet decade), season (was the timing right for orchids or not?)) of the of the main previous surveys, and even possibly the expertise-at-survey-time of the observers. Nothing will replace the quality of the results from such a process. Using online taxonomic tools such as the [GBIF taxonomy backbone](https://www.gbif.org/dataset/d7dddbf4-2cf0-4f39-9b2a-bb099caae36c) provides an automated alternative that comes with an unquantified (but unlikely to be negligible) penalty in quality. However, the enormous benefit in flexibility and time provided by automated tools makes them essential in practice.

Reconciling taxonomy can be done with the `make_taxa_taxonomy` function. This returns a list with two objects:

* `lutaxa.` This is a simple lookup from the original name provided to the taxa to use in place of that name. Thus, `taxa` is likely to be duplicated
* `taxa_taxonomy` provides the full taxonomic hierarchy for each `taxa` in the `lutaxa` object. The column `taxa` should not have any duplicates. The output can also include lifespan and indigenous status columns, if they are available from any of the original data sources and they are provided to `make_taxa_taxonomy`.

```{r make_taxa_taxonomy}

  taxa <-  make_taxa_taxonomy(flor_geo_bin
                              , out_file = "inst/luGBIF.rds"
                              )

```

A look at `lutaxa`.

```{r lutaxa}

  taxa$lutaxa

```

and `taxa_taxonomy`

```{r taxa_taxonomy}

  taxa$taxa_taxonomy

```

## Filter taxonomy

Cleaning to a single taxonomy is now possible sing `lutaxa` and `taxa_taxonomy`.

At this point, the [context](#context) is also _exclusively_ applied.

Thus the output of applying `filter_taxa` is to generate a single record for each taxa within each context. An attempt will also be made to provide a single value for any `extra_cols` but this will fail to provide a unique result if there are values within any of `extra_cols` that are not equivalent within a taxa and context.

```{r filt_tax}

  flor_taxa <- filter_taxa(flor_geo_bin
                          , taxa_col = "original_name"
                          , context = context
                          , taxonomy = taxa
                          ) %>%
    add_time_stamp()

  # Have a look at results
  flor_taxa

```

# Singletons

An implicit assumption in collation of data sources is usually that records were a list of taxa collected at a specific spatial location (and date). In many cases this assumption proves incorrect. For example, records may be related to, say, tree health monitoring where no other taxa were concurrently recorded.

Thus, filtering 'singleton' sites (a site with only a single taxa was recorded) is often prudent.

This will also inadvertently filter legitimate single taxa lists. For example, some areas of samphire or mangroves may have only a single taxa recorded within a survey site. As always, it depends on the goal of any particular analysis whether this trade-off will be worthwhile.

```{r flor_single}
  
  # Filter singletons
  flor_single <- filter_counts(flor_taxa
                              , context = context
                              , thresh = 1
                              ) %>%
    add_time_stamp()

  # Have a look at results
  flor_single

```

# Effort

Aggregated data sets are likely to contain records of taxa at spatial locations collected by almost any method imaginable. The effort any observer(s) put into time and taxonomy at each spatial location is usually unknown.

Given that effort matters when documenting ecology [e.g. @RN2377], filtering 'effort' is an attempt to remove the most seriously under- and over-sampled contexts.

Examples of low taxa richness contexts that may occur in large, unstructured data sets:

* brief, opportune records
* tree-health monitoring data

Examples of high taxa richness contexts that may occur in large, unstructured data sets:

* an observer wandering widely from the location they recorded, particularly if crossing an ecotone
* several observers working together
* an observer with well above average botanical knowledge
* observations taken over a long time-frame but recorded on a single day

These examples just scratch the surface of the ways in which taxa richness can deviate from that expected from an average effort by an average observer.

Taxa richness can be modelled as a function of any variables of interest, for example:

* nothing. This option might be useful in relatively small areas where the non-biological drivers of biological change are relatively stable
* a continuous variable of interest. Say, annual mean temperature, or, soil ph
* principal components. This option is best suited to analyses accompanied by many variables all of which are drivers, or closely correlated with, biological change
* a categorical variable of interest. Say, [IBRA](https://www.awe.gov.au/agriculture-land/land/nrs/science/ibra) Subregions

Taxa richness can be modelled using a Bayesian generalised linear model using the `make_effort_mod` function. This implements the `rstan_glm` function in the `rstanarm` package `r envReport::cite_package("rstanarm", bib_file = pac_cits)`. The resulting model is used to predict upper and lower bounds for acceptable taxa richness, by choosing acceptable lower and upper percentiles or richness.

```{r flor_effort}

  # run model
  effort_mod <- make_effort_mod(flor_single
                                , context = context
                                )

  # filter using model results
  flor_effort <- flor_single %>%
    dplyr::inner_join(effort_mod$mod_cell_result %>%
                        dplyr::filter(keep)
                      ) %>%
    dplyr::select(names(flor_single)) %>%
    add_time_stamp()

  # Have a look at results
  flor_effort

```

# Proportion of sites at which a taxa occurs

When clustering biological data, it is often useful to exclude rare taxa from the data set. `filter_prop` allows filtering such taxa.

```{r flor_prop}
  
  # Filter taxa recorded at less than 5% of contexts
  flor_prop <- filter_prop(flor_effort
                           , context = context
                           , default_per = 5
                           ) %>%
    add_time_stamp()
  
  # Have a look at results
  flor_prop

```

# Time

So far, all contexts `r vec_to_sentence(context, end = "or")` have been kept. It may be desirable to keep only the most recent contexts. Just use `dplyr`...

```{r flor_recent}

  flor_recent <- flor_prop %>%
    dplyr::group_by(across(any_of(context[!context %in% c("month", "year")]))) %>%
    dplyr::filter(year == max(year)
                  , month == max(month)
                  ) %>%
    dplyr::ungroup() %>%
    add_time_stamp()

  # Have a look at results
  flor_recent

```

# Filtering summary {#summary}

A little care naming objects enables easily capturing the results of the cleaning process. The function `cleaning_text` summarises the taxa, records, visits and sites in each `flor_` data frame.

```{r filt_summary}
  
  filt_summary <- cleaning_text(prefix = "flor_"
                                , save_ends = FALSE
                                , site_cols = c("cell", "lat", "long")
                                , visit_cols = c("month", "year")
                                , taxa_cols = c("taxa", "original_name")
                                )

  
```

```{r child_rmd}

  out <- NULL

  # this loops through child.Rmd, generating the results for each report card

  for (i in filt_summary$childID) {
    
    childID <- i
    
    out = c(out, knit_expand("inst/rmd/child.Rmd"))
    
  }

```

`r if(!is.null(out)) paste(knit(text = out), collapse = '\n')`


# What happened to x?

It can often be useful to trace the results for a single taxa through the filtering process. How did, say, _Eucalyptus incrassata_ fare through the process?

```{r find_taxa}

  find_taxa(taxa = "Eucalyptus incrassata"
            , lookup_taxa = taxa$lutaxa
            )

```

# Summary

After ingesting data from a range of sources (`r vec_to_sentence(unique(flor_all$data_name))`) the `envClean` functions helped clean the data ready for further analysis. The original `r filt_summary[1,"records"]` records were cleaned, tidied and filtered to `r filt_summary[nrow(filt_summary),"records"]` records.

# References
